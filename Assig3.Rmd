---
title: "MIS40970 Data Mining - Assignment 3 Classification"
output:
  html_notebook: default
  pdf_document: default
---
```{r}
#rm() is used to remove other objects from the environment
rm(list=ls())

#To check the working directory
getwd()
#A specific working drectory needs to be set for the loading of dataset
setwd("G:/R_programs_git/R_Progams/Classification/Classification")

```
## **Q2 Describe what this piece of R code is doing and why it is an important starting point for running classification algorithm. 
##> set.seed(1234)
##> dataPartition <- sample(2,nrow(data),replace=TRUE,prob=c(0.7,0.3))
##> trainData <- data[dataPartition ==1,]
##> testData <- [dataPartition ==2,]**

Classification needs supervised learning where we need to partition the data which can be used to train dataset. To explain the above set of commands, data has been imported from college.csv file as shown below:
```{r}
#Importing library readr to load data from csv file
library(readr)

#Importing data from CSV in a variable college
college <- read_csv("G:/R_programs_git/R_Progams/Classification/Classification/college.csv")

set.seed(1234)
dataPartition <- sample(2,nrow(college),replace=TRUE,prob=c(0.7,0.3))
trainCollege <- college[dataPartition ==1,]
testCollege <- college[dataPartition ==2,]
```
For the first statement(**set.seed(1234)**), we use seed number as a starting point that is used to generate a sequence of pseudo random numbers. This function is important if there is need that results should be reproducible and debuggable easily. 

Second statement(**dataPartition<-sample(2,nrow(data),replace=TRUE,prob=c(0.7,0.3))**) represents the properties of the partitions that need to be taken. Sample function takes a specified size from 2(an integer vector that can take one or more elements) so that function can generate random permutation of elements of vector(1:vector). If vector is 4, then the random permutation sequence will take numbers between 1 and 4 numbers. nrow(data) represents the size giving the numbers of items to choose and nrow is the last row of the college dataset. replace represents whether the sampling should be done with replacement or without replacement. Replace=TRUE is set to do the sampling with replacement. And prob will take probability weights for obtaining sampled elements. So it is expected that 1 will appear more than 2 times as weight is 0.7 than 0.3. If we write prob=c(0.3,0.5,0.2) then 1 is appeared to be less times than 2 but more than 3. 

**trainCollege <- college[dataPartition ==1,] and testCollege <- college[dataPartition ==2,]** split the college dataset into 2 datasets i.e. Test data and Train data, where test data is used for testing. 

All these commands will help in creating training dataset that can be changed continuously that help in formulation of classification algorithms. 

## **Q3 What is the role of the M parameter in the Weka implementation of C4.5 algorithm? Which part of the DTL induction process does this parameter affect?**


## **Q4. Install	R	package	"C50".	Import	customer	churn	dataset	(churn)	using	data() function. Examine	the	churnTrain dataset. Using	R	run	a	decision-tree	classification	algorithm	of	your choice	constructing	a	full	unpruned	tree	and	a	pruned	tree.	Compare	classification	results	of the	pruned	and	unpruned	trees generated.**	
```{r}
install.packages('C50')

library(C50)

#To examine datasets in package C50
data()
```
After examining data function, it has been found out that there are two datasets in 'Customer Churn Data' (C50 package), which are churnTest and churnTrain. To import datasets in C50 package, use command "data(churn)".
```{r}
data(churn)
print("Details of attributes in churnTrain")
print("___________________________________")
str(churnTrain)

print("---------------------------Summary Table :churnTrain---------------------")
#To understand dataset churnTrain
summary(churnTrain)

churnTrain <- churnTrain[,c(-1,-4)]
print("Details of attributes in churnTrain after selecting few columns")
print("_______________________________________________________________")
str(churnTrain)
```
```{r}
set.seed(1234)
dataPart <- sample(2, nrow(churnTrain),replace = TRUE,prob = c(0.7,0.3))
traindata <- churnTrain[dataPart ==1,]
testdata <- churnTrain[dataPart ==2,]

#Installing package 'tree' for Decision Tree

install.packages("tree")

library(tree)

```
```{r}
#Grow a tree using tree to predict customer churn for all other independent variables

fit <- tree(traindata$churn ~ .,traindata)

#To print detailed summary of splits
summary(fit)
cat("\n \n")

#Prediction using predict function for both traindata and testdata
traindata_C50 = predict(fit,traindata,type="class")
testdata_C50 = predict(fit,testdata,type="class")

#Printing prediction result using table
print("Prediction of traindata")
cat("_______________________\n")
table(traindata_C50,traindata$churn)
cat("\n")

print("Prediction of testdata")
cat("______________________\n")
table(testdata_C50,testdata$churn)

#To plot the tree
plot(fit)
text(fit, all = TRUE, cex = 0.5)
```
```{r}
#Pruning the tree
pfit = cv.tree(fit,FUN = prune.misclass)
print("Details of prune.misclass")
print("_________________________")
cat("\n")
pfit

pruneData = prune.misclass(fit,best=6)
#Plot the pruned tree
plot(pruneData)
text(pruneData,all = TRUE, cex = 0.5)

pruneData2 = prune.misclass(fit,best=2)
#Plot the pruned tree
plot(pruneData2)
text(pruneData2,all = TRUE, cex = 0.7)
```
Pruning the decision tree will help to avoid overfitting the data. It will minimize the cross validation error (uing xerror) and select complexity parameter that is associated with least error. Results shows that unpruned tree is larger because the algorithm is implemented as is. While in pruned tree, there is an additional step which analyse which nodes or branches to be removed that will not affect the performance of decision tree. 

## **Q5 Compare generalisation performance of the pruned and unpruned tree from Q4. Output relevant summaries and confusion matrices. Describe the results.**



